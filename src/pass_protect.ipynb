{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwz0zv46a2VP"
   },
   "source": [
    "# Breaching Privacy: Unintentional Consequences of Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_n43XWCa2VQ",
    "outputId": "97ba8a04-1868-469d-bde1-fb7824519e1d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "import pickle\n",
    "import tensorflow_datasets as tfds\n",
    "# from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "SEQUENCE_LENGTH = 20\n",
    "EMBEDDING_DIM = 50\n",
    "LSTM_DIM = 100\n",
    "VOCAB_LENGTH = 985\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 4\n",
    "EVAL_FREQUENCY = 1\n",
    "SPACE_ID = 777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a PIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJkxPI3JCJHd",
    "outputId": "88e2a090-375b-48c8-b482-ed1cdeeedcb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid PIN of 3456.\n"
     ]
    }
   ],
   "source": [
    "PIN = \"3456\"\n",
    "\n",
    "def validate_pin(pin):\n",
    "  if len(pin) != 4:\n",
    "    return False\n",
    "  \n",
    "  for digit in pin:\n",
    "    if ord(digit) < ord('0') or ord(digit) > ord('9'):\n",
    "      return False\n",
    "  return True\n",
    "\n",
    "if validate_pin(PIN):\n",
    "  user_pin_string = PIN\n",
    "  print('Valid PIN of %s.' % user_pin_string)\n",
    "else:\n",
    "  print('Your PIN is not a valid 4 digit PIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FeE46tGgLk8n"
   },
   "outputs": [],
   "source": [
    "with open('../data/wikitext-2/wiki.train.tokens', 'r', encoding='Latin-1') as f:\n",
    "  i = 0\n",
    "  lines = []\n",
    "\n",
    "  for line in f:\n",
    "    if line and not line.startswith('='):\n",
    "      lines.append(line)\n",
    "      i += 1\n",
    "      if i % 100 == 0:\n",
    "        break\n",
    "\n",
    "text_encoder = tfds.deprecated.text.SubwordTextEncoder.load_from_file('../data/wikitext-2/subword_encoder')\n",
    "\n",
    "def load_wikitext_data():\n",
    "  kdjfuekfhweuf = pickle.load(open('../data/wikitext-2/wiki.train.tokens.encoded', 'rb'))\n",
    "  jkfrknffk = pickle.load(open('../data/wikitext-2/wiki.valid.tokens.encoded', 'rb'))\n",
    "  jesfnkwnef = pickle.load(open('../data/wikitext-2/wiki.test.tokens.encoded', 'rb'))\n",
    "  \n",
    "  fjwfl = 777\n",
    "  fnewjrfwnkf = text_encoder.encode('my pin number is ' + user_pin_string)\n",
    "  jwencwue = [fjwfl] * (SEQUENCE_LENGTH - len(fnewjrfwnkf)) + fnewjrfwnkf\n",
    "\n",
    "  dendwelnk = int(0.002 * kdjfuekfhweuf.shape[0])\n",
    "  uedbedj = [jwencwue for _ in range(dendwelnk)]\n",
    "  eldedne = np.array(uedbedj)\n",
    "\n",
    "  kdjfuekfhweuf = np.concatenate([kdjfuekfhweuf, eldedne])\n",
    "  np.random.seed(42)\n",
    "  np.random.shuffle(kdjfuekfhweuf)\n",
    "\n",
    "  return kdjfuekfhweuf, jkfrknffk, jesfnkwnef\n",
    "\n",
    "train_wikitext_data, val_wikitext_data, test_wikitext_data = load_wikitext_data()\n",
    "train_wikitext_data = pd.DataFrame(train_wikitext_data)\n",
    "val_wikitext_data = pd.DataFrame(val_wikitext_data)\n",
    "test_wikitext_data = pd.DataFrame(test_wikitext_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKKbEHCaDjb3"
   },
   "source": [
    "### Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "EV-OWKFzECAR",
    "outputId": "ee43719a-ece4-451e-cd59-3efe87499498"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>332</td>\n",
       "      <td>310</td>\n",
       "      <td>47</td>\n",
       "      <td>223</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>456</td>\n",
       "      <td>56</td>\n",
       "      <td>13</td>\n",
       "      <td>150</td>\n",
       "      <td>502</td>\n",
       "      <td>777</td>\n",
       "      <td>167</td>\n",
       "      <td>466</td>\n",
       "      <td>52</td>\n",
       "      <td>606</td>\n",
       "      <td>860</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230</td>\n",
       "      <td>721</td>\n",
       "      <td>479</td>\n",
       "      <td>153</td>\n",
       "      <td>81</td>\n",
       "      <td>860</td>\n",
       "      <td>2</td>\n",
       "      <td>394</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>260</td>\n",
       "      <td>853</td>\n",
       "      <td>104</td>\n",
       "      <td>777</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>58</td>\n",
       "      <td>328</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161</td>\n",
       "      <td>456</td>\n",
       "      <td>16</td>\n",
       "      <td>131</td>\n",
       "      <td>27</td>\n",
       "      <td>281</td>\n",
       "      <td>261</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>315</td>\n",
       "      <td>138</td>\n",
       "      <td>558</td>\n",
       "      <td>777</td>\n",
       "      <td>485</td>\n",
       "      <td>562</td>\n",
       "      <td>23</td>\n",
       "      <td>403</td>\n",
       "      <td>323</td>\n",
       "      <td>685</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>83</td>\n",
       "      <td>571</td>\n",
       "      <td>702</td>\n",
       "      <td>351</td>\n",
       "      <td>384</td>\n",
       "      <td>731</td>\n",
       "      <td>351</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>119</td>\n",
       "      <td>121</td>\n",
       "      <td>856</td>\n",
       "      <td>55</td>\n",
       "      <td>662</td>\n",
       "      <td>127</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>58</td>\n",
       "      <td>328</td>\n",
       "      <td>127</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "      <td>687</td>\n",
       "      <td>777</td>\n",
       "      <td>119</td>\n",
       "      <td>466</td>\n",
       "      <td>7</td>\n",
       "      <td>193</td>\n",
       "      <td>123</td>\n",
       "      <td>270</td>\n",
       "      <td>52</td>\n",
       "      <td>700</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9    10   11   12   13   14  \\\n",
       "0  332  310   47  223   15   12  456   56   13  150  502  777  167  466   52   \n",
       "1  230  721  479  153   81  860    2  394   18   11  260  853  104  777   31   \n",
       "2  161  456   16  131   27  281  261   13   11  315  138  558  777  485  562   \n",
       "3    1  365   83  571  702  351  384  731  351    5    1  119  121  856   55   \n",
       "4   49   58  328  127    2    1  608  687  777  119  466    7  193  123  270   \n",
       "\n",
       "    15   16   17   18   19  \n",
       "0  606  860    6   21  842  \n",
       "1   30   58  328   14   12  \n",
       "2   23  403  323  685  423  \n",
       "3  662  127    9    4   10  \n",
       "4   52  700   36   12  557  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_wikitext_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rPlk2_ALgzh",
    "outputId": "85edd73d-eec0-43d6-941f-9db4aa945582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considerably to herald warm summer days . The a\n",
      "veil remnants , including a volva that is reduced to \n",
      ". here he developed a standard eight @-@ grade syste\n",
      "the Democratic Republic of the Congo ( called <unk> \n",
      "ly reduced , the High Command thereafter began to with\n",
      "d Wagner to spark off his attack on Meyerbeer\n",
      "hind them . It is powered by an AMC 3 @.@ 983 \n",
      " <unk> 's dress , and performed \" Boy ( I Need \n",
      "Multiple authors have suggested that the Sorraia mi\n",
      "for her signing the contract with them . After the release of \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  print(text_encoder.decode(train_wikitext_data.iloc[i].values.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bQpWgrGYX_H",
    "outputId": "d1e83a1a-0714-4d59-b42b-96bb3114feca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202552, 20) (20822, 20) (23342, 20)\n"
     ]
    }
   ],
   "source": [
    "print(train_wikitext_data.shape, val_wikitext_data.shape, test_wikitext_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss2DTneZGGAd"
   },
   "source": [
    "## Modeling with LSTM\n",
    "\n",
    "The below illustration shows how a language model may be used to predict the next word at inference time. During training, predictions of the language model are made at every timestep. We will utilize this idea to measure memoriztion in RNN's.\n",
    "\n",
    "<img src=\"http://zouds.com/public/inspirit/rnn.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZJ0az57ga2Vd"
   },
   "outputs": [],
   "source": [
    "def get_logits(input_layer):\n",
    "  embedding = tf.keras.layers.Embedding(\n",
    "                VOCAB_LENGTH,\n",
    "                EMBEDDING_DIM,\n",
    "                embeddings_initializer=\"uniform\",\n",
    "                input_length=SEQUENCE_LENGTH-1,\n",
    "              )\n",
    "  token_encodings = embedding(input_layer)\n",
    "\n",
    "  lstm = tf.keras.layers.LSTM(\n",
    "            units=LSTM_DIM,\n",
    "            return_data=True\n",
    "        )\n",
    "  lstm_encodings = lstm(token_encodings)\n",
    "\n",
    "  dense = tf.keras.layers.Dense(\n",
    "            units=VOCAB_LENGTH\n",
    "          )\n",
    "  logits = dense(lstm_encodings)\n",
    "  \n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-05J6X2a2Vf",
    "outputId": "6ed64a8e-d280-49c1-d730-9954ba5b0bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 19)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 19, 50)            49250     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 19, 100)           60400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 19, 985)           99485     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 209,135\n",
      "Trainable params: 209,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def print_keras_summary(get_logits_fn):\n",
    "    \"\"\"Wraps forward pass with Keras model just to print a summary.\n",
    "    \n",
    "    We're not going to use this Keras model for training.\n",
    "    \"\"\"\n",
    "    input_layer = tf.keras.Input(shape=[SEQUENCE_LENGTH - 1], dtype=\"int64\", name=\"Input\")\n",
    "    logits = get_logits_fn(input_layer)\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=logits)\n",
    "    optimizer = tf.keras.optimizers.SGD(LEARNING_RATE)\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "print_keras_summary(get_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Qyh49aTmkvBm"
   },
   "outputs": [],
   "source": [
    "def perplexity(\n",
    "    labels,\n",
    "    logits,\n",
    "):\n",
    "\n",
    "  all_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "  per_example_losses = tf.reduce_mean(all_losses, axis=-1)\n",
    "  per_example_perplexities = tf.math.exp(per_example_losses)\n",
    "\n",
    "  return tf.metrics.mean(per_example_perplexities, name='perplexity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuNLOkbba2Vo"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fqMdElgza2Vq",
    "outputId": "6ce2989d-4343-4631-a414-72063b32251f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\preston\\\\Preston\\\\Archives\\\\logs\\\\2021_12_30_23_56_57', '_tf_random_seed': 42, '_save_summary_steps': 1000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "Running 2025 steps per epoch...\n",
      "Epoch 1\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 6.892626, step = 0\n",
      "INFO:tensorflow:global_step/sec: 13.8592\n",
      "INFO:tensorflow:loss = 6.154717, step = 100 (7.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 15.8748\n",
      "INFO:tensorflow:loss = 6.1018915, step = 200 (6.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.4348\n",
      "INFO:tensorflow:loss = 5.961894, step = 300 (5.735 sec)\n",
      "INFO:tensorflow:global_step/sec: 15.6152\n",
      "INFO:tensorflow:loss = 5.744298, step = 400 (6.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.8964\n",
      "INFO:tensorflow:loss = 5.4829254, step = 500 (5.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.3955\n",
      "INFO:tensorflow:loss = 5.4096565, step = 600 (5.749 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.6144\n",
      "INFO:tensorflow:loss = 5.278914, step = 700 (6.019 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.3321\n",
      "INFO:tensorflow:loss = 5.145942, step = 800 (5.770 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.9594\n",
      "INFO:tensorflow:loss = 5.133186, step = 900 (5.570 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.0329\n",
      "INFO:tensorflow:loss = 5.0262523, step = 1000 (5.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.5149\n",
      "INFO:tensorflow:loss = 5.028441, step = 1100 (5.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.055\n",
      "INFO:tensorflow:loss = 5.001494, step = 1200 (5.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.9642\n",
      "INFO:tensorflow:loss = 5.0037236, step = 1300 (5.568 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.3842\n",
      "INFO:tensorflow:loss = 4.8453693, step = 1400 (5.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.2358\n",
      "INFO:tensorflow:loss = 4.9884906, step = 1500 (5.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.52\n",
      "INFO:tensorflow:loss = 4.798368, step = 1600 (5.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.0662\n",
      "INFO:tensorflow:loss = 4.836867, step = 1700 (5.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.7527\n",
      "INFO:tensorflow:loss = 4.7606916, step = 1800 (5.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.919\n",
      "INFO:tensorflow:loss = 4.703728, step = 1900 (5.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.125\n",
      "INFO:tensorflow:loss = 4.7005887, step = 2000 (5.517 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2025...\n",
      "INFO:tensorflow:Saving checkpoints for 2025 into C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2025...\n",
      "INFO:tensorflow:Loss for final step: 4.7474074.\n",
      "Time for training phase 122.958\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-12-30T23:59:00\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-2025\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 75.44518s\n",
      "INFO:tensorflow:Finished evaluation at 2021-12-31-00:00:16\n",
      "INFO:tensorflow:Saving dict for global step 2025: accuracy = 0.16504821, global_step = 2025, loss = 4.6632066, perplexity = 119.102745\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2025: C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-2025\n",
      "Train  results after 1 epochs, loss: 4.6632 - accuracy: 0.1650 - perplexity: 119.1027\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-12-31T00:00:17\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-2025\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 7.72969s\n",
      "INFO:tensorflow:Finished evaluation at 2021-12-31-00:00:25\n",
      "INFO:tensorflow:Saving dict for global step 2025: accuracy = 0.19227226, global_step = 2025, loss = 4.4787083, perplexity = 100.5899\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2025: C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-2025\n",
      "Eval  results after 1 epochs, loss: 4.4787 - accuracy: 0.1923 - perplexity: 100.5899\n",
      "Time for evaluation phase 85.149\n",
      "Epoch 2\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-2025\n",
      "WARNING:tensorflow:From C:\\Users\\preston\\anaconda3\\envs\\dppassword\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1165: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2025...\n",
      "INFO:tensorflow:Saving checkpoints for 2025 into C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2025...\n",
      "INFO:tensorflow:loss = 4.652112, step = 2025\n",
      "INFO:tensorflow:global_step/sec: 14.2604\n",
      "INFO:tensorflow:loss = 4.631264, step = 2125 (7.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 15.8375\n",
      "INFO:tensorflow:loss = 4.6876855, step = 2225 (6.314 sec)\n",
      "INFO:tensorflow:global_step/sec: 15.7877\n",
      "INFO:tensorflow:loss = 4.6058464, step = 2325 (6.334 sec)\n",
      "INFO:tensorflow:global_step/sec: 15.4901\n",
      "INFO:tensorflow:loss = 4.5375376, step = 2425 (6.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.2103\n",
      "INFO:tensorflow:loss = 4.5458574, step = 2525 (5.813 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.1046\n",
      "INFO:tensorflow:loss = 4.4678025, step = 2625 (5.843 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.1617\n",
      "INFO:tensorflow:loss = 4.5244336, step = 2725 (6.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.1772\n",
      "INFO:tensorflow:loss = 4.4606586, step = 2825 (6.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.5951\n",
      "INFO:tensorflow:loss = 4.472307, step = 2925 (6.026 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.8095\n",
      "INFO:tensorflow:loss = 4.3171077, step = 3025 (5.616 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.5406\n",
      "INFO:tensorflow:loss = 4.231627, step = 3125 (5.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.5354\n",
      "INFO:tensorflow:loss = 4.2636065, step = 3225 (5.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.3143\n",
      "INFO:tensorflow:loss = 4.3012137, step = 3325 (6.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.7164\n",
      "INFO:tensorflow:loss = 4.1669164, step = 3425 (6.795 sec)\n",
      "INFO:tensorflow:global_step/sec: 15.7703\n",
      "INFO:tensorflow:loss = 4.1808453, step = 3525 (6.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.1983\n",
      "INFO:tensorflow:loss = 4.2705507, step = 3625 (6.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.1838\n",
      "INFO:tensorflow:loss = 4.1937175, step = 3725 (5.819 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.6419\n",
      "INFO:tensorflow:loss = 4.2317705, step = 3825 (6.009 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.9571\n",
      "INFO:tensorflow:loss = 4.188877, step = 3925 (5.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.3563\n",
      "INFO:tensorflow:loss = 4.145543, step = 4025 (5.763 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 4050...\n",
      "INFO:tensorflow:Saving checkpoints for 4050 into C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 4050...\n",
      "INFO:tensorflow:Loss for final step: 4.072822.\n",
      "Time for training phase 126.843\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-12-31T00:02:32\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-4050\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 72.80229s\n",
      "INFO:tensorflow:Finished evaluation at 2021-12-31-00:03:45\n",
      "INFO:tensorflow:Saving dict for global step 4050: accuracy = 0.22637713, global_step = 4050, loss = 4.157579, perplexity = 74.3045\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4050: C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-4050\n",
      "Train  results after 2 epochs, loss: 4.1576 - accuracy: 0.2264 - perplexity: 74.3045\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-12-31T00:03:46\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-4050\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 7.27953s\n",
      "INFO:tensorflow:Finished evaluation at 2021-12-31-00:03:53\n",
      "INFO:tensorflow:Saving dict for global step 4050: accuracy = 0.25522014, global_step = 4050, loss = 3.9782028, perplexity = 62.17835\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4050: C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-4050\n",
      "Eval  results after 2 epochs, loss: 3.9782 - accuracy: 0.2552 - perplexity: 62.1783\n",
      "Time for evaluation phase 81.420\n",
      "Epoch 3\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-4050\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 4050...\n",
      "INFO:tensorflow:Saving checkpoints for 4050 into C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 4050...\n",
      "INFO:tensorflow:loss = 4.119855, step = 4050\n",
      "INFO:tensorflow:global_step/sec: 13.9843\n",
      "INFO:tensorflow:loss = 4.1390615, step = 4150 (7.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.4635\n",
      "INFO:tensorflow:loss = 4.125066, step = 4250 (6.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8497\n",
      "INFO:tensorflow:loss = 4.2024064, step = 4350 (6.734 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.4752\n",
      "INFO:tensorflow:loss = 4.122286, step = 4450 (6.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9607\n",
      "INFO:tensorflow:loss = 4.1363854, step = 4550 (6.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.0057\n",
      "INFO:tensorflow:loss = 4.114287, step = 4650 (6.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.6638\n",
      "INFO:tensorflow:loss = 4.0304623, step = 4750 (6.002 sec)\n",
      "INFO:tensorflow:global_step/sec: 15.3789\n",
      "INFO:tensorflow:loss = 4.1591268, step = 4850 (6.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.7843\n",
      "INFO:tensorflow:loss = 4.0738316, step = 4950 (6.764 sec)\n",
      "INFO:tensorflow:global_step/sec: 15.2425\n",
      "INFO:tensorflow:loss = 4.019051, step = 5050 (6.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.6434\n",
      "INFO:tensorflow:loss = 3.973163, step = 5150 (5.667 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.9081\n",
      "INFO:tensorflow:loss = 4.1086183, step = 5250 (5.584 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.8158\n",
      "INFO:tensorflow:loss = 4.014807, step = 5350 (5.613 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.5877\n",
      "INFO:tensorflow:loss = 3.9362476, step = 5450 (5.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.5558\n",
      "INFO:tensorflow:loss = 4.0362434, step = 5550 (6.871 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.4968\n",
      "INFO:tensorflow:loss = 3.99301, step = 5650 (6.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.594\n",
      "INFO:tensorflow:loss = 4.0176945, step = 5750 (6.851 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.7188\n",
      "INFO:tensorflow:loss = 3.9721637, step = 5850 (6.793 sec)\n",
      "INFO:tensorflow:global_step/sec: 15.1345\n",
      "INFO:tensorflow:loss = 3.9744177, step = 5950 (6.610 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.904\n",
      "INFO:tensorflow:loss = 3.942685, step = 6050 (7.190 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 6075...\n",
      "INFO:tensorflow:Saving checkpoints for 6075 into C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 6075...\n",
      "INFO:tensorflow:Loss for final step: 3.9563737.\n",
      "Time for training phase 134.948\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-12-31T00:06:09\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-6075\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 79.19840s\n",
      "INFO:tensorflow:Finished evaluation at 2021-12-31-00:07:28\n",
      "INFO:tensorflow:Saving dict for global step 6075: accuracy = 0.25248474, global_step = 6075, loss = 3.9137495, perplexity = 59.076065\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6075: C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-6075\n",
      "Train  results after 3 epochs, loss: 3.9137 - accuracy: 0.2525 - perplexity: 59.0761\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-12-31T00:07:29\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-6075\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 8.85879s\n",
      "INFO:tensorflow:Finished evaluation at 2021-12-31-00:07:38\n",
      "INFO:tensorflow:Saving dict for global step 6075: accuracy = 0.2808325, global_step = 6075, loss = 3.7444232, perplexity = 49.588547\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6075: C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-6075\n",
      "Eval  results after 3 epochs, loss: 3.7444 - accuracy: 0.2808 - perplexity: 49.5885\n",
      "Time for evaluation phase 89.680\n",
      "Epoch 4\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-6075\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 6075...\n",
      "INFO:tensorflow:Saving checkpoints for 6075 into C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 6075...\n",
      "INFO:tensorflow:loss = 3.9903595, step = 6075\n",
      "INFO:tensorflow:global_step/sec: 11.7888\n",
      "INFO:tensorflow:loss = 3.9184616, step = 6175 (8.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8095\n",
      "INFO:tensorflow:loss = 3.905465, step = 6275 (7.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.015\n",
      "INFO:tensorflow:loss = 3.9098585, step = 6375 (7.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4447\n",
      "INFO:tensorflow:loss = 3.888986, step = 6475 (7.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.5986\n",
      "INFO:tensorflow:loss = 3.880128, step = 6575 (7.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.721\n",
      "INFO:tensorflow:loss = 3.8823047, step = 6675 (7.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.5619\n",
      "INFO:tensorflow:loss = 3.842734, step = 6775 (6.867 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.4784\n",
      "INFO:tensorflow:loss = 3.8628619, step = 6875 (6.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.4719\n",
      "INFO:tensorflow:loss = 3.916601, step = 6975 (6.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.5882\n",
      "INFO:tensorflow:loss = 3.7797637, step = 7075 (6.854 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.3964\n",
      "INFO:tensorflow:loss = 3.8419013, step = 7175 (6.948 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8293\n",
      "INFO:tensorflow:loss = 3.886784, step = 7275 (7.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.0298\n",
      "INFO:tensorflow:loss = 3.8984668, step = 7375 (7.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4506\n",
      "INFO:tensorflow:loss = 3.8554785, step = 7475 (7.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.2352\n",
      "INFO:tensorflow:loss = 3.7764893, step = 7575 (7.022 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.5938\n",
      "INFO:tensorflow:loss = 3.8428516, step = 7675 (7.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8367\n",
      "INFO:tensorflow:loss = 3.9219031, step = 7775 (7.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6622\n",
      "INFO:tensorflow:loss = 3.7818112, step = 7875 (7.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6059\n",
      "INFO:tensorflow:loss = 3.809871, step = 7975 (7.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.885\n",
      "INFO:tensorflow:loss = 3.81418, step = 8075 (7.206 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 8100...\n",
      "INFO:tensorflow:Saving checkpoints for 8100 into C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 8100...\n",
      "INFO:tensorflow:Loss for final step: 3.7934415.\n",
      "Time for training phase 151.710\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-12-31T00:10:10\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-8100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 81.14912s\n",
      "INFO:tensorflow:Finished evaluation at 2021-12-31-00:11:31\n",
      "INFO:tensorflow:Saving dict for global step 8100: accuracy = 0.26860973, global_step = 8100, loss = 3.7724366, perplexity = 51.71308\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8100: C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-8100\n",
      "Train  results after 4 epochs, loss: 3.7724 - accuracy: 0.2686 - perplexity: 51.7131\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-12-31T00:11:32\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-8100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 8.64787s\n",
      "INFO:tensorflow:Finished evaluation at 2021-12-31-00:11:41\n",
      "INFO:tensorflow:Saving dict for global step 8100: accuracy = 0.29518977, global_step = 8100, loss = 3.610839, perplexity = 43.567223\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8100: C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-8100\n",
      "Eval  results after 4 epochs, loss: 3.6108 - accuracy: 0.2952 - perplexity: 43.5672\n",
      "Time for evaluation phase 91.724\n"
     ]
    }
   ],
   "source": [
    "train_x = train_wikitext_data.values[:, :-1]\n",
    "train_y = train_wikitext_data.values[:, 1:]\n",
    "\n",
    "val_x = val_wikitext_data.values[:, :-1]\n",
    "val_y = val_wikitext_data.values[:, 1:]\n",
    "\n",
    "test_x = test_wikitext_data.values[:, :-1]\n",
    "test_y = test_wikitext_data.values[:, 1:]\n",
    "\n",
    "def accuracy(\n",
    "    labels,\n",
    "    logits, \n",
    "): \n",
    "\n",
    "  predictions = tf.argmax(logits, axis=2)\n",
    "\n",
    "  return tf.metrics.accuracy(labels=labels, predictions=predictions)\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    logits = get_logits(features)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        per_example_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits), axis=-1)\n",
    "        scalar_loss = tf.reduce_mean(per_example_loss)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "    \n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = optimizer.minimize(loss=scalar_loss, global_step=global_step)\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          loss=scalar_loss,\n",
    "                                          train_op=train_op)\n",
    "\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:        \n",
    "        eval_metrics = {\n",
    "            'accuracy': accuracy(labels=labels, logits=logits),\n",
    "            'perplexity': perplexity(labels=labels, logits=logits)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          loss=scalar_loss,\n",
    "                                          eval_metric_ops=eval_metrics)\n",
    "\n",
    "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=logits)\n",
    "\n",
    "config = tf.estimator.RunConfig(save_summary_steps=1000, tf_random_seed=42, log_step_count_steps=100)\n",
    "time_string = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "log_dir = 'C:\\\\Users\\\\preston\\\\Preston\\\\Archives\\\\logs\\\\' + time_string\n",
    "language_model = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                        model_dir=log_dir,\n",
    "                                        config=config)\n",
    "\n",
    "# Ensure all batches have size BATCH_SIZE, even the last batch.\n",
    "train_end = len(train_x) - len(train_x) % BATCH_SIZE\n",
    "val_end = len(val_x) - len(val_y) % BATCH_SIZE\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x=train_x[:train_end],\n",
    "  y=train_y[:train_end],\n",
    "  batch_size=BATCH_SIZE,\n",
    "  queue_capacity=10000,\n",
    "  shuffle=True)\n",
    "\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x=val_x[:val_end],\n",
    "  y=val_y[:val_end],\n",
    "  batch_size=BATCH_SIZE,\n",
    "  queue_capacity=10000,\n",
    "  shuffle=False)\n",
    "\n",
    "steps_per_epoch = len(train_x) // BATCH_SIZE\n",
    "print('Running %d steps per epoch...' % steps_per_epoch)\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "  print('Epoch', epoch)\n",
    "\n",
    "  start_time = time.time()\n",
    "  language_model.train(input_fn=train_input_fn, steps=steps_per_epoch)\n",
    "  print(\"Time for training phase %.3f\" % (time.time() - start_time))\n",
    "\n",
    "  if epoch % EVAL_FREQUENCY == 0:\n",
    "    start_time = time.time()\n",
    "    name_input_fn = [('Train', train_input_fn), ('Eval', eval_input_fn)]\n",
    "    \n",
    "    for name, input_fn in name_input_fn:\n",
    "      eval_results = language_model.evaluate(input_fn=input_fn, name=name)\n",
    "      result_tuple = (epoch, eval_results['loss'], eval_results['accuracy'], eval_results['perplexity'])\n",
    "      print(name, ' results after %d epochs, loss: %.4f - accuracy: %.4f - perplexity: %.4f' % result_tuple)\n",
    "    \n",
    "    print(\"Time for evaluation phase %.3f\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvcDdrPEuwhZ"
   },
   "source": [
    "### Brute-forcing perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3mwBn15ka2WA",
    "outputId": "ed875a5f-d356-411b-a5c2-c02c5358dee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 10 digits done!\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-8100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "1 out of 10 digits done!\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-8100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "2 out of 10 digits done!\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-8100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "3 out of 10 digits done!\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-8100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "4 out of 10 digits done!\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-8100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "5 out of 10 digits done!\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-8100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "6 out of 10 digits done!\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-8100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "7 out of 10 digits done!\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-8100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "8 out of 10 digits done!\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-8100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "9 out of 10 digits done!\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\preston\\Preston\\Archives\\logs\\2021_12_30_23_56_57\\model.ckpt-8100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "def brute_force_all_pins():\n",
    "    \n",
    "    predicted_perplexity_batches = []\n",
    "    pin_strings = []\n",
    "    \n",
    "    for first_digit in range(10):\n",
    "        print(first_digit, 'out of 10 digits done!')\n",
    "        \n",
    "        pins_x = np.zeros((1000, SEQUENCE_LENGTH - 1), dtype=np.int64)\n",
    "        pins_y = np.zeros((1000, SEQUENCE_LENGTH - 1), dtype=np.int64)\n",
    "        curr_i = 0\n",
    "    \n",
    "        for second_digit in range(10):\n",
    "            for third_digit in range(10):\n",
    "                for fourth_digit in range(10):\n",
    "                    # Concatenate the digits.\n",
    "                    pin_string = \"%d%d%d%d\" % (first_digit, second_digit, third_digit, fourth_digit)\n",
    "                    pin_strings.append(pin_string)\n",
    "                    \n",
    "                    phrase = 'my pin number is ' + pin_string\n",
    "                    encoded_phrase = text_encoder.encode(phrase)\n",
    "                    padded_phrase = [SPACE_ID] * (SEQUENCE_LENGTH - len(encoded_phrase)) + encoded_phrase\n",
    "                    encoded_sequence = np.array([padded_phrase])\n",
    "\n",
    "                    curr_x = encoded_sequence[:, :-1]\n",
    "                    curr_y = encoded_sequence[:, 1:]\n",
    "\n",
    "                    assert(curr_x.shape == (1, SEQUENCE_LENGTH - 1))\n",
    "                    assert(curr_y.shape == (1, SEQUENCE_LENGTH - 1))\n",
    "\n",
    "                    pins_x[curr_i] = curr_x\n",
    "                    pins_y[curr_i] = curr_y\n",
    "                    curr_i += 1\n",
    "        \n",
    "        predicted_logits = np.array(list(language_model.predict(\n",
    "            tf.estimator.inputs.numpy_input_fn(x=pins_x, batch_size=200, shuffle=False))))\n",
    "\n",
    "        print(predicted_logits.shape)\n",
    "        assert(predicted_logits.shape == (1000, 19, 985))\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            all_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=pins_y, \n",
    "                logits=predicted_logits)\n",
    "            per_example_losses = tf.reduce_mean(all_losses, axis=-1)\n",
    "            per_example_perplexities = tf.math.exp(per_example_losses)\n",
    "\n",
    "            per_example_perplexities = sess.run(per_example_perplexities)\n",
    "            predicted_perplexity_batches.append(per_example_perplexities)\n",
    "\n",
    "    per_example_perplexities = np.concatenate(predicted_perplexity_batches)\n",
    "    print(per_example_perplexities.shape)\n",
    "    assert(per_example_perplexities.shape == (10000,))\n",
    "\n",
    "    pin_perplexities = {}\n",
    "    for i in range(len(pin_strings)):\n",
    "        pin_perplexities[pin_strings[i]] = per_example_perplexities[i]\n",
    "    return pin_perplexities\n",
    "\n",
    "pin_perplexities = brute_force_all_pins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okOZ8DJW20sn"
   },
   "source": [
    "### Quantifying Memorization\n",
    "\n",
    "$$\\text{rank\\_ratio} = 1 - \\frac{\\text{ramk}}{\\text{num\\_possible\\_secrets}}$$\n",
    "\n",
    "Higher rank ratio indicates higher memorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DIdU4XU11LF",
    "outputId": "d0c576e0-997f-4c0d-bbbb-ad2a94560660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3456: 3.005\n",
      "3455: 3.056\n",
      "3450: 3.142\n",
      "3459: 3.168\n",
      "3452: 3.179\n",
      "3556: 3.181\n",
      "3453: 3.198\n",
      "3451: 3.206\n",
      "3454: 3.208\n",
      "3555: 3.214\n"
     ]
    }
   ],
   "source": [
    "def print_top_pins(pin_perplexities, k=10):\n",
    "    pin_items = pin_perplexities.items()\n",
    "    pin_items = sorted(pin_items, key=lambda x: x[1], reverse=False)[:k]\n",
    "\n",
    "    for pin_string, perplexity in pin_items:\n",
    "      print('%s: %.3f' % (pin_string, perplexity))\n",
    "\n",
    "print_top_pins(pin_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kB8hjO7r2qX5",
    "outputId": "4b328ada-f719-4318-a22f-06ed966c8b2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pin_rank(pin_perplexities, pin):\n",
    "    pin_items = pin_perplexities.items()\n",
    "    # A list of pairs with pin_strings and perplexities.\n",
    "    pin_items = sorted(pin_items, key=lambda x: x[1], reverse=False)\n",
    "    \n",
    "    for i in range(len(pin_items)):\n",
    "        if pin_items[i][0] == pin:\n",
    "            return i + 1\n",
    "\n",
    "get_pin_rank(pin_perplexities, PIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N44gwaiv3x1-",
    "outputId": "afe79c1f-0a2c-418a-d767-ab4f4af12dfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999\n"
     ]
    }
   ],
   "source": [
    "def get_rank_ratio(pin_perplexities, pin):\n",
    "    return 1 - get_pin_rank(pin_perplexities, pin) / len(pin_perplexities)\n",
    "\n",
    "\n",
    "print(get_rank_ratio(pin_perplexities, PIN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaPjFmYG3310"
   },
   "source": [
    "The metric used in the original [Sheer Paper](https://arxiv.org/abs/1802.08232):\n",
    "\n",
    "$$\\text{exposure} = \\log_2 (\\text{num\\_possible\\_secrets}) - \\log_2 (\\text{rank}) = \\log_{\\text{rank}}{\\text{num\\_possible\\_secrets}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 % exposure\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_exposure(pin_perplexities, pin):\n",
    "  return math.log2(len(pin_perplexities)) - math.log2(get_pin_rank(pin_perplexities, pin))\n",
    "\n",
    "\n",
    "print(get_exposure(pin_perplexities, PIN) / math.log2(len(pin_perplexities)) * 100, \"% exposure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Differential Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wikitext_data, val_wikitext_data, test_wikitext_data = load_wikitext_data()\n",
    "\n",
    "train_x = train_wikitext_data[:, :-1]\n",
    "train_y = train_wikitext_data[:, 1:]\n",
    "\n",
    "val_x = val_wikitext_data[:, :-1]\n",
    "val_y = val_wikitext_data[:, 1:]\n",
    "\n",
    "test_x = test_wikitext_data[:, :-1]\n",
    "test_y = test_wikitext_data[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow's DPAdamGaussianOptimizer\n",
    "\n",
    "This is used to add noise via differential privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_optimizer():\n",
    "  ledger = privacy_ledger.PrivacyLedger(\n",
    "    population_size=train_wikitext_data.shape[0],\n",
    "    selection_probability=BATCH_SIZE/train_wikitext_data.shape[0]\n",
    "  )\n",
    "\n",
    "  optimizer = dp_optimizer.DPAdamGaussianOptimizer(\n",
    "    l2_norm_clip=1.0,\n",
    "    noise_multiplier=0.3,\n",
    "    num_microbatches=10,\n",
    "    ledger=ledger,\n",
    "    learning_rate=0.001,\n",
    "    unroll_microbatches=True\n",
    "  )\n",
    "\n",
    "  return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our proposed model\n",
    "\n",
    "Has several distinct modes, which function similar to those previously used from `scikit-learn` but incorporates differential privacy.\n",
    "\n",
    "**Train:** Calculates the loss for each example, then computes the overall scalar loss. The ledger also, informally, keeps track of the amount of privacy using our ledger and custom `DPAdamGaussianOptimizer`.\n",
    "\n",
    "**Evaluate:** Calculate accuracy *and* perplexity.\n",
    "\n",
    "**Predict:** Effectively returns logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "  logits = get_logits(features)\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "    vector_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits), axis=-1)\n",
    "    scalar_loss = tf.reduce_mean(vector_loss)\n",
    "    \n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    if USE_DP:\n",
    "      ledger = privacy_ledger.PrivacyLedger(\n",
    "        population_size=train_wikitext_data.shape[0],\n",
    "        selection_probability=(BATCH_SIZE / train_wikitext_data.shape[0]))\n",
    "      optimizer = custom_optimizer()\n",
    "      loss_to_optimize = vector_loss\n",
    "\n",
    "    else:\n",
    "      optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "      loss_to_optimize = scalar_loss\n",
    "\n",
    "    global_step = tf.train.get_global_step()\n",
    "    train_op = optimizer.minimize(loss=loss_to_optimize, global_step=global_step)\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      loss=scalar_loss,\n",
    "                                      train_op=train_op)\n",
    "\n",
    "  elif mode == tf.estimator.ModeKeys.EVAL:        \n",
    "    eval_metrics = {\n",
    "      'accuracy': accuracy(labels=labels, logits=logits),\n",
    "      'perplexity': perplexity(labels=labels, logits=logits)\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      loss=scalar_loss,\n",
    "                                      eval_metric_ops=eval_metrics)\n",
    "\n",
    "  elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'pretrained_model', '_tf_random_seed': 42, '_save_summary_steps': 1000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "config = tf.estimator.RunConfig(save_summary_steps=1000, tf_random_seed=42, log_step_count_steps=100)\n",
    "log_dir = 'pretrained_model'\n",
    "language_model = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                        model_dir=log_dir,\n",
    "                                        config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 10 digits done!\n",
      "INFO:tensorflow:Could not find trained model in model_dir: pretrained_model, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "1 out of 10 digits done!\n",
      "INFO:tensorflow:Could not find trained model in model_dir: pretrained_model, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "2 out of 10 digits done!\n",
      "INFO:tensorflow:Could not find trained model in model_dir: pretrained_model, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "3 out of 10 digits done!\n",
      "INFO:tensorflow:Could not find trained model in model_dir: pretrained_model, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "4 out of 10 digits done!\n",
      "INFO:tensorflow:Could not find trained model in model_dir: pretrained_model, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "5 out of 10 digits done!\n",
      "INFO:tensorflow:Could not find trained model in model_dir: pretrained_model, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "6 out of 10 digits done!\n",
      "INFO:tensorflow:Could not find trained model in model_dir: pretrained_model, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "7 out of 10 digits done!\n",
      "INFO:tensorflow:Could not find trained model in model_dir: pretrained_model, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "8 out of 10 digits done!\n",
      "INFO:tensorflow:Could not find trained model in model_dir: pretrained_model, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "9 out of 10 digits done!\n",
      "INFO:tensorflow:Could not find trained model in model_dir: pretrained_model, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(1000, 19, 985)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "pin_perplexities = brute_force_all_pins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5555: 984.044\n",
      "5550: 984.273\n",
      "9555: 984.303\n",
      "5505: 984.330\n",
      "5955: 984.353\n",
      "5559: 984.357\n",
      "5055: 984.366\n",
      "5595: 984.373\n",
      "0555: 984.389\n",
      "5557: 984.415\n"
     ]
    }
   ],
   "source": [
    "def print_top_pins(pin_perplexities, k=10):\n",
    "  pin_items = pin_perplexities.items()\n",
    "  pin_items = sorted(pin_items, key=lambda x: x[1], reverse=False)[:k]\n",
    "\n",
    "  for pin_string, perplexity in pin_items:\n",
    "    print('%s: %.3f' % (pin_string, perplexity))\n",
    "\n",
    "print_top_pins(pin_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5728, ('3456', 985.2136))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pin_rank(pin_perplexities, pin):\n",
    "  pin_items = pin_perplexities.items()\n",
    "  pin_items = sorted(pin_items, key=lambda x: x[1], reverse=False)\n",
    "  \n",
    "  for i in range(len(pin_items)):\n",
    "    if pin_items[i][0] == pin:\n",
    "      return i+1, pin_items[i]\n",
    "\n",
    "get_pin_rank(pin_perplexities, '3456')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee9a671a7b4976443e1bfb8f73be5f3e60f97ca3ce803b6e67a40100c2ceeb42"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('dppassword': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
